---
title: "World Happiness Survey Analysis"
author: "Christian Berardi"
date: "October 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(scatterplot3d)
library(glmnet)
library(randomForest)
library(e1071)
library(knitr)
library(kableExtra)
library(dendextend)
```

##Background

Following the adoption of UN Resolution 65/309 in 2011, the UN has released, with the exception of 2014, yearly reports on global happiness.
\footnote {Happiness: towards a holistic approach to development: resolution adopted by the General Assembly, http://repository.un.org/handle/11176/291712} 
These Happiness Reports are a result of global surveys conducted under the auspicious of the UN.
\footnote {World Happiness Report, http://worldhappiness.report/faq/} 
A number of different variables, along with happiness are included in the World Happiness Report. 
\footnote{World Happiness Report 2017, http://worldhappiness.report/ed/2017/}
For the purpose of this analysis, the following variables will be used as predictors for a country's happiness: Economy, Family, Health, Freedom, Generosity, Trust. 


###Variables
Each variable will now be defined.
\footnote{World Happiness Report Statistical Appendix, https://s3.amazonaws.com/happiness-report/2017/StatisticalAppendixWHR2017.pdf}

####Happiness
The target, happiness, is the national average of ranking, from 0 to 10, of how well a person believes their life is going, from the worst possible, to the best possible.

####Economy
Economy is a measure of national GDP in PPP per capita in constant 2011 dollars in August of 2016. 

####Family
Family is the average of an indicator measuring whether or not the respondent has someone, a friend or family member, who would help them in a time of personal crisis.

####Health
Health is a measure of healthy life expectancy at birth, not simply life expectancy at birth.

####Freedom
Freedom is the average of an indicator of whether or not the respondent is satisfied with their freedom to choose what to do in life.

####Generosity
Generosity is a measure of the frequency of charitable donation per month scaled with GDP per capita.

####Trust
Trust is a measure of corruption perception, the average between two indicators: existence of widespread public corruption and existence of widespread private corruption. 

###Analysis
In order to better understand the relationship between the different predictors, the data will first be explored. This exploration will look for any problems with the data itself, either missing or questionable values. Correlations between the predictors will be calculated to determine if multicolinearity will be an issue with this data. 

Following that, the relationships between the predictors and the relationships between the nations investigated in the survey, will be measured using various unstructured learning techniques. Principle Component Analysis (PCA), will be done to better understand the relationships between the different predictors, and to serve as predictors if multicolinearity proves intractable. A Bi-Plot will then be created to identify outlier nations that require special consideration and interpretation. Following that, Hierarchical Clustering (HC) will be done to help to identify patterns among the nations in the survey, these patterns will also be identified through K- Means Clustering analysis.

Subsequent to unstructed learning, structured learning, namely three predictive modeling algorithms, with be done to predict happiness using the 6 predictors identified for this analysis. 3 models will be fit to the data: LASSO, Random Forest (RF), and Support Vector Machine (SVM). These three techniques will undergo hyperparameter optimization, model, validation, and finally model comparison to the select the best fit for the target. Once the optimum model is found, it will be interpreted and conclusions from the entire analysis. 

\pagebreak

##Data Exploration
In order to better understand the relationship between the different predictors, as well as to be begin to be able to appropriately model the data, some basic statistics, as well as correlation, and the relationship between pairs of predictors will be explored. 
```{r}
data <- read.csv("2017.csv")

#Remove rank, wiskers from data, dystopia
happy <- cbind(data[,1],data[,3],data[,6:11])
#Rename the column names to something shorter and more meaningful
colnames(happy) <-c("Country", "Happiness", "Economy", "Family", "Health", "Freedom"
                    , "Generosity", "Trust" )
```


From Table 1, the data contains 155 different nations. Looking at the summary statistics calculated for each of the 6 predictors used in the report, none have missing values, nor do they have maximum values much greater than their 3rd quartile. For this reason no work need be done on data cleaning. This is expected of a data set from Kaggle.


```{r} 
kable(summary(happy), "latex", caption="Summary of Data", booktabs=T) %>%
  kable_styling(latex_options=c("scale_down", "hold_position")) 
```

\pagebreak

###Correlation Between Predictors
```{r}
plot(happy[,-1])
```
The plot above shows clear evidence of high correlation between multiple predictors. For this reason controlling multicolinearity will need to be a focus when modeling the data. 

\pagebreak

Looking further into correlation, we find the same issue when calculating the correlation between the predictors. Table 2 shows correlation between many of the predictors is large.

This correlation indicates many predictors carry the same information, which exposes the multicolinearity risk inherent in modeling this data.

```{r}
kable(cor(happy[,-1]), caption="Correlation between Predictors", booktabs=T)
```

\pagebreak

##Unstructed Learning
To obtain a better understanding of the nations and relationships between the correlated predictors, as well as to discover outliers and cluster the nations of the world, PCA, HC, K-Means Clustering  were carried out and Bi-Plots were created. 

###Principle Component Analysis
```{r}
#Do PCA and clustering to get a better handle on the relationship between the predictors and happiness
pca <- princomp(happy[,3:length(happy)], cor=TRUE) #Since vars are not standardized used cor=TRUE
#pca$loadings
#summary(pca)
eigs <- pca$sdev^2
prop <- eigs/sum(eigs)
plot(prop, xlab="Number of Component", ylab="Proportion of Variance Explained") #So only 3 needed, now interpret
```
The scree plot indicates that 3 principle components(PC) needs to be interpreted. The 1^st^ principle component is:
```{r}
kable(pca$loading[,1], caption="PC 1", booktabs=T)
```

This PC indicates a weighted average of the different predictors. This gives us no special insight into the relationship between the predictors. 

\pagebreak

The second and third PCs are:
```{r}
kable(pca$loading[,2], caption="PC 2", booktabs=T)
kable(pca$loading[,3], caption= "PC 3", booktabs=T)
```

These PCs are much more interesting. The 2^nd^ PC is a contrast between economy, family and health on one hand and freedom, generosity and trust on the other. This can be interpreted as suggesting that there are two classes of predictors for this data, one I will call goodness of life predictors: economy, family and health, the other goodness of government: freedom, generosity and trust. The 3^rd^ PC moves generosity from the goodness of government to goodness of life predictors. The conclusions that can be drawn form this will be considered following bi-plot analysis.

\pagebreak

###Bi-Plot
```{r}
#Do a Biplot to start to look at more relationships between predictors
biplot(pca)
```
To check the conclusions from the (PCA), a bi-plot was constructed. The bi-plot results are very similar to those seen from PCA, the goodness of life predictors are clustered, as are two of the goodness of government: trust, freedom. Generosity is now found on its own, which is consistent with its change from one contrast to the other seen in PCA.
MORE

```{r}
biplot(pca, xlabs=happy[,1])
```
A second bi-plot was constructed to determine which countries could be seen as outliers so that they could be better understood. We immediately see a number of countries which deserve further scrutiny: Myanmar, Rwanda, Somalia, Greece, Lithuania, and Taiwan (hidden by the goodness of life predictors).

####Myanmar
Myanmar is ranked low in term of happiness, 114 out of 155. Its outlier nature is due to its much higher then expected generosity score given its happiness-- it has the highest score. The most likely explanation for this discrepancy is the highly Therevadan Buddhist nature of the majority Bamar ethnic group who, by the definition of the predictor, must be very willing to donate to charity.

####Rwanda
Rwanda is also ranked very low in terms of happiness, 151 out of 155. Its outlier nature is due to its very high trust score given its low happiness. This is most likely due to the after effects of the Rwandan genocide, and the extreme changes it brought to the Rwandan government, eliminating a large amount of the corruption endemic to the region.

####Somalia
Somalia is ranked near the middle in terms of happiness, 93. However, Somalia's economy score is second to last, which would usually indicate an extremely low happiness rating. The reason for this difference is most likely the continued factional nature of the Somali state. It might also indicate a need to address the adequacy of economic indicators in Somalia, and leads to questions about their accuracy. 

####Greece
Greece has a middling happiness level, ranked 87, which is in sharp contrast to its generosity, ranked last, which would usually indicate a much lower happiness. This is most likely a consequence of the continued financial instability of the country resulting a lack of willingness to give to charity when an individual is already experiencing a decline in living standards.

####Lithuania
Lithuania is ranked in the top third of countries, which is surprising give its low generosity score, only slightly higher than Greece's. This would normally indicate a much less happy nation. Lithuania, like Greece, also recently went through financial instability. For this reason its interpretation matches that of the Hellenic State.

####Taiwan
Taiwan is near the top of the rankings in terms of happiness, ranked 33. However its low freedom score would normally be associated with a much lower happiness. The reasons for this perception of a lack of ability to choose what one does in life would require further research to fully understand. 

###Hierarchical Clustering
```{r}
#hierarchical clustering
happy_h <-hclust(dist(happy[,3:length(happy)]), method="complete")
happy_h$labels <- happy$Country
dend <- as.dendrogram(happy_h)
dend <-  color_labels(happy_h, k = 3)
#Comment out to prevent stray output in pdf
#png(file="Dendrogram_plot.png", width=10, height=24, units="in", res=288)
#par(mar=c(2,2,2,10))
#plot(dend, horiz=T, main="Dendrogram of HC with 3 Clusters")
#dev.off()
```
![](dendrogram_plot.png)
\pagebreak

HC was done to better understand the relationships between the different countries, that is, to see if there was some way to classify the countries based on their predictor values. To that end complete clustering was done which produced the dendrogram seen on the previous page.

Interpreting the results, it is the second split that is most meaningful. This splits the countries into three groups: those with very low happiness, pink, green, the second branch, and those with middle happiness, blue. 

This 3-way split corresponds, in some way, to the old Cold War, 3 World classification. However, it should be noted that the "3^rd^ World" has greatly shrunk, while the 2^nd^ World, previously the domain of communist regimes, has now become the most numerous. The 1st world has also increased in size, taking former 2^nd^ and 3^rd^ World nations.

\pagebreak

###K-Means Clustering
```{r}
#k-means clustering
happy_k <- kmeans(happy[,3:length(happy)], 3)
#calculate the first 3 PC
pcc1 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,1]
pcc2 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,2]
pcc3 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,3]
#Create a matrix with the PCs and the component membership number
PCs=cbind(pcc1,pcc2,pcc3, happy_k$cluster)

#Plots the first three PCs by cluster with different colors
cluster_1 <- PCs[PCs[,4]==1,]
cluster_2 <- PCs[PCs[,4]==2,]
cluster_3 <- PCs[PCs[,4]==3,]
splt <- scatterplot3d(cluster_1[,1],cluster_1[,2],cluster_1[,3], xlab="PC1", ylab="PC2", zlab="PC3", color = "blue",
                      xlim=c(-2.5,0),ylim=c(-1,1),zlim=c(.2,-.6))
splt$points3d(cluster_2[,1],cluster_2[,2],cluster_2[,3], col="red")
splt$points3d(cluster_3[,1],cluster_3[,2],cluster_3[,3], col="green")

#Put the clusters in as a variable in a new data set to be used in visualizations in tableau
country_clusters <- cbind(happy[,1], happy_k$cluster)
colnames(country_clusters) <- c("Country","Cluster")
write.csv(country_clusters, file="clusters2017.csv")
```
This division of the data into 3 parts is consistent with the results from PCA, HC also indicates there is some evidence in the data of a tripartite division in the world. For this reason, K-means clustering was done with a cluster number of 3. 

The plot above shows the results of this clustering. Further visualization and interpretation of the cluster will done using Tableau

![](clusters.png)
This plot from Tableau shows a clear indication of the meaning of the 3 clusters identified above. One cluster corresponds to the highest rated countries (green), one the lowest (red), and one to everyone else (yellow). These results correspond, almost completely, with the current developed, developing and under developed nations of the world.

\pagebreak

##Structured Learning
```{r}
#Since muilticolinearity is an issue, fit 3 models that can help deal with it: linear regression with the LASSO, #random forest and SVM, do LOOCV to determine optimum model compare models based on MSE to select the optimum # model, interpret results in light of how to get the happiest country

#Implement hyperparamter optimization after implementing train/test splitting
#Implemented overfitting check

#Set the number of Train/Test iterations to do
its<-1000
```

###Regression
```{r}
#Use the LASSO to help mitigate multicolinearity
lasso_MSE <- matrix(ncol = 2, nrow=its)
colnames(lasso_MSE) <- c("Train", "Test")

#Use cv.glmnet to find optimum tuning parameter lambda with 5-fold cross validation
lasso_5fold <- cv.glmnet(as.matrix(happy[,3:length(happy)]), happy[,2],nfold=5, family='gaussian')
#plot(lasso_5fold$lambda,lasso_5fold$cvm)
#Record optimum lambda
lambda <- lasso_5fold$lambda.min

for (i in 1:its){
  index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
  train_x <- happy[index,3:length(happy)]
  train_y <- happy[index,2]
  test_x <- happy[-index,3:length(happy)]
  test_y <- happy[-index,2]
  lasso <- glmnet(as.matrix(train_x),train_y, family="gaussian", lambda=lambda)
  
#Testing for overfitting
  preds <- predict(lasso, as.matrix(train_x))
  error <- preds-train_y
  lasso_MSE[i,1] <- mean(error^2)
    
  preds <- predict(lasso, as.matrix(test_x))
  error <- preds-test_y
  lasso_MSE[i,2] <- mean(error^2)
}


```
In order to help overcome issues with the multicolinearity inherent in the data, the LASSO will be used to obtain a linear model of the data. Using 5-fold cross validation to select the optimum regularization parameter, $\lambda$, for fitting a LASSO model, the following LASSO model was fit to the data. `r its` 70/30 train test splits were performed to estimate the MSE and standard deviation of the MSE for both the training and test data. The results are as follows:
```{r}
kable(apply(lasso_MSE, 2, mean),caption="LASSO MSE Means", booktabs=T)
```
```{r}
kable(apply(lasso_MSE, 2, sd), caption="LASSO MSE Std.", booktabs=T)
```

\pagebreak

###Random Forest
```{r}
#Hyperparamter optimization: number of tree in model
trees <- c(50,100,250,500,1000,1500,2000)

for (j in 1:length(trees)){
  #Do 1000 70/30 train test splits, average over results to get an idea of ASE
  #Create containers for MSE and importance
  forest_MSE <- matrix(ncol = 2, nrow=its)
  colnames(forest_MSE) <- c("Train", "Test")

  forest_importance <- matrix(nrow=its, ncol=length(happy[,3:length(happy)]))
  colnames(forest_importance) <- c("Economy", "Family", "Health", "Freedom"
                    , "Generosity", "Trust")
  for (i in 1:its){
    index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
    train_x <- happy[index,3:length(happy)]
    train_y <- happy[index,2]
    test_x <- happy[-index,3:length(happy)]
    test_y <- happy[-index,2]
    forest <- randomForest(x=train_x ,y=train_y, xtest=test_x, ytest=test_y,ntree=trees[j], importance=TRUE)
  
    error <- forest$predicted-train_y
    forest_MSE[i,1] <- mean(error^2)
  
    error <- forest$test$predicted-test_y
    forest_MSE[i,2] <- mean(error^2)
  
    forest_importance[i,] <- forest$importance[,1]
  }
  this_forest <- c(apply(forest_MSE, 2, mean), apply(forest_MSE, 2, sd), apply(forest_importance, 2, mean), apply(forest_importance, 2, sd))
  if (j ==1 ){
    forest_hyper <- data.frame(this_forest)
  } else{
   forest_hyper <- data.frame(forest_hyper, this_forest) 
  }
}
forest_hyper <- data.frame(forest_hyper, row.names= c("Mean MSE Train", "Mean MSE Test", "Mean MSE Sd Train", "Mean MSE Sd Test", "Economy", "Family", "Health", "Freedom", "Generosity", "Trust", "Economy Sd", "Family Sd", "Health Sd", "Freedom Sd", "Generosity Sd", "Trust Sd"))
colnames(forest_hyper) <- trees

```
While an RF is said to be self-validating, given the nature of the algorithm, `r its` 70/30 train/test splits were done to estimate the MSE and $\sigma$ of the estimates. 500 trees were used. Once hyperparameter optimization is coded, the number of trees will be determined by optimization from the fit of the data. The results are as follows:

```{r}
kable(forest_hyper, caption="Random Forest Results", booktabs=T)

```


Furthermore, variable importance was extracted from the random forest model. The results are as follows:

It is clear from Tables 10 and 11 that Health, Economy, Family and Freedom, in that order play an important role in predicting happiness in a country. Generosity and Trust appear to lack much predictive power. Interpreting these results, the most important factor in predicting happiness is how long and healthy an individual is during their life. After that how wealthy, then how much support, then how free an individual is to make choices, round out the most important predictors. 

REWRITE TO WORK WITH NEW CHART

If we were to engage in sophomoric psychology, we would find these correspond rather well to Maslow's Hierarchy of needs. \footnote{Maslow, A (1954). Motivation and personality. New York, NY: Harper.} The most important need is that of corporeal safety, i.e., safety in one's body. Following that economic safety, the ability to earn enough money to support one's self. Then moving on to the ability to find aid in times of trouble, and at last being free to choose how one uses their life. 

\pagebreak

###Support Vector Machine
```{r}
support_MSE <- matrix(ncol = 2, nrow=its)
colnames(support_MSE) <- c("Train", "Test")
eps <- c(.001,.01,.1,.5,1)

#Try a radial SVM to start, look at others during hyperparameter optimization
#Radial kernel overfits, try a different one. Linear fits better and doesn't overfit
for (j in 1:length(eps)){
  for (i in 1:its){
    index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
    train_x <- happy[index,3:length(happy)]
    train_y <- happy[index,2]
    test_x <- happy[-index,3:length(happy)]
    test_y <- happy[-index,2]
    support <- svm(x=train_x ,y=train_y, kernel= "linear")
  
    support_MSE[i,1] <- mean(support$residuals^2)
  
    preds <- predict(support, test_x)
    error <- preds-test_y
    support_MSE[i,2] <- mean(error^2)
  }
  this_eps <- c(apply(support_MSE, 2, mean), apply(support_MSE, 2, sd))
  if (j==1){
    SVM_hyper <- data.frame(this_eps)
  } else{
    SVM_hyper <- data.frame(SVM_hyper, this_eps)
  }
}
SVM_hyper <- data.frame(SVM_hyper, row.names= c("Mean MSE Train", "Mean MSE Test", "Mean MSE Sd Train", "Mean MSE Sd Test"))
colnames(SVM_hyper) <- eps
```
Two SVMs were fit to the data. The first was an SVM with radial kernel. This model was found to overfit the data and so was replaced with a model with linear kernel. This model did not overfit the data. `r its` 70/30 train/test splits were done to estimate the MSE and the $\sigma$ of the estimates. No hyperparameter optimization has as of yet been coded.

The following results were found:
```{r}
kable(SVM_hyper, caption="Support Vector Machine Results", booktabs=T)
```

\pagebreak

###Optimum Model


\pagebreak

##Conclusions
