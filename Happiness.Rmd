---
title: "World Happiness Survey Analysis"
author: "Christian Berardi"
date: "October 10, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(scatterplot3d)
library(glmnet)
library(randomForest)
library(e1071)
```

##Background
Add in stuff about the survey etc. describe the data and the predictors as well as the target

\pagebreak

##Data Exploration
In order to better understand the relationship between the different predictors, as well as to be begin to be able to appropriately model the data, some basic statistics, as well as correlation, and the relationship between pairs of predictors will be explored. 
```{r}
data <- read.csv("2017.csv")

#Remove rank, wiskers from data, dystopia
happy <- cbind(data[,1],data[,3],data[,6:11])
#Rename the column names to something shorter and more meaningful
colnames(happy) <-c("Country", "Happiness", "Economy", "Family", "Health", "Freedom"
                    , "Generosity", "Trust" )
summary(happy)
```
The data contains 155 different countries. Looking at the summary statistics calculated for each of the 6 predictors used in the report, none have missing values, nor do they have maximum values much greater than their 3rd quartile. For this reason no work need be done on data cleaning. This was expected of a data set from Kaggle.

\pagebreak

###Correlation Between Predictors
```{r}
plot(happy[,-1])
```
The plot above shows clear evidence of high correlation between multiple predictors. For that reason controlling multicolinearity will need to be a focus when modeling the data. 

\pagebreak

```{r}
cor(happy[,-1])
```
Looking further, we find the same issue when calculating the correlation between the predictors. The correlation between economy and health, as well as between

To combat the multicolinearity, a number of different approaches with be attempted, including variable selection as well as modeling with principle components. 

\pagebreak
##Unstructed Learning
To obtain a better understanding of the relationship between the correlated predictors, as well as to discover outlier countries and cluster the countries of the world, principle component analysis, hierarchical clustering, k-means clustering and bi-plots were carried out. 

###Principle Component Analysis
```{r}
#Do PCA and clustering to get a better handle on the relationship between the predictors and happiness
pca <- princomp(happy[,3:length(happy)], cor=TRUE) #Since vars are not standardized used cor=TRUE
pca$loadings
summary(pca)
eigs <- pca$sdev^2
prop <- eigs/sum(eigs)
plot(prop, xlab="Number of Component", ylab="Proportion of Variance Explained") #So only 3 needed, now interpret
```
The scree plot indicates that 3 principle components(PC) needs to be interpreted. The first principle component (`r pca$loading[,1]`) indicated a weighted average of the different predictors. This gives us no special insight into their relationship. 

The second PC (`r pca$loading[,2]`) is much more interesting. This PC is a contrast between economy, family and health on one hand and freedom, generosity and trust on the other. This can be interpreted as suggesting that there are two classes of predictors for this data, one I will call goodness of life predictors: economy, family and health, the other goodness of government: freedom, generosity and trust. This indicated that we must be very careful in selecting more than one predictor from each class.

The final PC, `r pca$loading[,3]`, is another contrast, this time mixing the two classes that were identified from the second PC. The difference in classification is for generosity.

###Bi-Plot
```{r}
#Do a Biplot to start to look at more relationships between predictors
biplot(pca)
```
To check the conclusions from the principle component analysis(PCA), a bi-plot was constructed. The bi-plot results are very similar to those seen from PCA, the goodness of life predictors are clustered, as are the goodness of government, with generosity further from the other two goodness of government measures.

```{r}
biplot(pca, xlabs=happy[,1])
```
A second bi-plot was constructed to determine which countries could be seen as outliers so that they could be better understood. We immediately see a number of countries which deserve further scrutiny: Myanmar, Rwanda, Somalia, Greece, Lithuania, and Taiwan (hidden by the goodness of life predictors).

####Myanmar
Myanmar is ranked low, 114 out of 155. Its outlier nature is due to its much higher then expected generosity score given its happiness-- it has the highest score. The most likely explanation for this discrepancy is the highly Therevadan Buddhist nature of the majority Bamar ethnic group. 

####Rwanda
Rwanda is also ranked very low, 151 out of 155. It's outlier nature is due to its very high government score given its happiness, the second highest. This is most likely due to the after effects of the Rwandan genocide, and the extreme changes it brought to the Rwandan government.

####Somalia
Somalia is ranked near the middle in terms of happiness, 93. However, Somalia's economy score is second to last, which would usually indicate an extremely low happiness rating. The reason for this difference is most likely the continued factional nature of the Somali state.

####Greece
Greece has a middling happiness level, ranked 87, which is in sharp contrast to its generosity, ranked last, which would usually indicate a much lower happiness. This is most likely a consequence of the continued financial instability of the country.

####Lithuania
Lithuania is ranked in the top third of countries, which is surprising give its low generosity score, only slightly higher than Greece's. This would normally indicate a much less happy nation. Lithuania, like Greece, also recently went through financial instability.

####Taiwan
Taiwan is near the top of the rankings in terms of happiness, ranked 33. However its low freedom score would normally be associated with a much lower happiness. 

###Hierarchical Clustering
```{r}
#hierarchical clustering
happy_h <-hclust(dist(happy[,3:length(happy)]), method="complete")
happy_h$labels <-happy
#Comment out to prevent stray output in pdf
#png(file="Dendrogram_plot.png", width=10, height=24, units="in", res=288)
#par(mar=c(2,2,2,10))
#plot(as.dendrogram(happy_h), horiz=T)
#dev.off()
```
![](dendrogram_plot.png)
\pagebreak

Hierarchical Clustering (HC) was done to better understand the relationship between the different countries, that is, to see if there was some way to classify the countries based on their predictor values. To that end complete clustering was done which produced the dendrogram seen on the previous page.

Interpreting the results, it is the second split that is most meaningful. This splits the countries into three groups: those with very low happiness, the first branch from the bottom, those with high happiness, the second branch, and those with middle happiness, the top branch. 

This 3-way split corresponds, in some way, to the old Cold War, 3 world classification. However, it should be noted that the "3^rd^ world" has greatly shrunk, while the 2^nd^ world previously the domain of communist regimes, has now become the most numerous. The 1st world has also increased in size. 

###K-Means Clustering
```{r}
#k-means clustering
happy_k <- kmeans(happy[,3:length(happy)], 3)
#calculate the first 3 PC
pcc1 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,1]
pcc2 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,2]
pcc3 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,3]
#Create a matrix with the PCs and the component membership number
PCs=cbind(pcc1,pcc2,pcc3, happy_k$cluster)

#Plots the first three PCs by cluster with different colors
cluster_1 <- PCs[PCs[,4]==1,]
cluster_2 <- PCs[PCs[,4]==2,]
cluster_3 <- PCs[PCs[,4]==3,]
splt <- scatterplot3d(cluster_1[,1],cluster_1[,2],cluster_1[,3], xlab="PC1", ylab="PC2", zlab="PC3", color = "blue",
                      xlim=c(-2.5,0),ylim=c(-1,1),zlim=c(.2,-.6))
splt$points3d(cluster_2[,1],cluster_2[,2],cluster_2[,3], col="red")
splt$points3d(cluster_3[,1],cluster_3[,2],cluster_3[,3], col="green")

#Put the clusters in as a variable in a new data set to be used in visualizations in tableau
country_clusters <- cbind(happy[,1], happy_k$cluster)
colnames(country_clusters) <- c("Country","Cluster")
write.csv(country_clusters, file="clusters2017.csv")
```
This division of the data into 3 parts is consistent with the results from PCA as well as HC indicates there is some evidence in the data of a tripartite division in the world. For this reason, K-means clustering was done with a cluster number of 3. 

The plot above shows the results of this clustering. Further visualization and interpretation of the cluster will done using Tableau

\pagebreak

##Structured Learning
```{r}
#Since muilticolinearity is an issue, fit 3 models that can help deal with it: linear regression with the LASSO, #random forest and SVM, do LOOCV to determine optimum model compare models based on AIC and BIC to select the optimum # model, interpret results in light of how to get the happiest country
#Set the number of Train/Test iterations to do
its<-1000
```

###Regression
```{r}
#Use the LASSO to help mitigate multicolinearity

```
###Random Forest
```{r}
#Do 1000 70/30 train test splits, average over results to get an idea of ASE
#Create containers for MSE and importance
forest_MSE <- vector(length=its)
forest_importance <- matrix(nrow=its, ncol=length(happy[,3:length(happy)]))
colnames(forest_importance) <- c("Economy", "Family", "Health", "Freedom"
                    , "Generosity", "Trust")
for (i in 1:its){
  index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
  train_x <- happy[index,3:length(happy)]
  train_y <- happy[index,2]
  test_x <- happy[-index,3:length(happy)]
  test_y <- happy[-index,2]
  forest <- randomForest(x=train_x ,y=train_y, xtest=test_x, ytest=test_y,ntree=500, importance=TRUE)
  forest_MSE[i] <- mean(forest$mse)
  forest_importance[i,] <- forest$importance[,1]
}
mean(forest_MSE)
sd(forest_MSE)
apply(forest_importance, 2, mean)
apply(forest_importance, 2, sd)
```

###Support Vectorm Machine
```{r}

```
