---
title: "World Happiness Survey Analysis"
author: "Christian Berardi"
date: "October 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(scatterplot3d)
library(glmnet)
library(randomForest)
library(e1071)
library(knitr)
library(kableExtra)
```

##Background
Add in stuff about the survey etc. describe the data and the predictors as well as the target

\pagebreak

##Data Exploration
In order to better understand the relationship between the different predictors, as well as to be begin to be able to appropriately model the data, some basic statistics, as well as correlation, and the relationship between pairs of predictors will be explored. 
```{r}
data <- read.csv("2017.csv")

#Remove rank, wiskers from data, dystopia
happy <- cbind(data[,1],data[,3],data[,6:11])
#Rename the column names to something shorter and more meaningful
colnames(happy) <-c("Country", "Happiness", "Economy", "Family", "Health", "Freedom"
                    , "Generosity", "Trust" )
```


The data contains 155 different countries. Looking at the summary statistics calculated for each of the 6 predictors used in the report, none have missing values, nor do they have maximum values much greater than their 3rd quartile. For this reason no work need be done on data cleaning. This was expected of a data set from Kaggle.


```{r} 
kable(summary(happy), "latex", caption="Summary of Data", booktabs=T) %>%
  kable_styling(latex_options=c("scale_down", "hold_position")) 
```

\pagebreak

###Correlation Between Predictors
```{r}
plot(happy[,-1])
```
The plot above shows clear evidence of high correlation between multiple predictors. For that reason controlling multicolinearity will need to be a focus when modeling the data. 

\pagebreak

Looking further, we find the same issue when calculating the correlation between the predictors. The correlation between economy and health, as well as between

To combat the multicolinearity, a number of different approaches with be attempted, including variable selection as well as modeling with principle components. 

```{r}
kable(cor(happy[,-1]), caption="Correlation between Predictors", booktabs=T)
```

\pagebreak

##Unstructed Learning
To obtain a better understanding of the relationship between the correlated predictors, as well as to discover outlier countries and cluster the countries of the world, principle component analysis, hierarchical clustering, k-means clustering and bi-plots were carried out. 

###Principle Component Analysis
```{r}
#Do PCA and clustering to get a better handle on the relationship between the predictors and happiness
pca <- princomp(happy[,3:length(happy)], cor=TRUE) #Since vars are not standardized used cor=TRUE
#pca$loadings
#summary(pca)
eigs <- pca$sdev^2
prop <- eigs/sum(eigs)
plot(prop, xlab="Number of Component", ylab="Proportion of Variance Explained") #So only 3 needed, now interpret
```
The scree plot indicates that 3 principle components(PC) needs to be interpreted. The first principle component:
```{r}
kable(pca$loading[,1], caption="PCA 1", booktabs=T)
```

indicates a weighted average of the different predictors. This gives us no special insight into their relationship. 

The second PC:
```{r}
kable(pca$loading[,2], caption="PCA 2", booktabs=T)
```

is much more interesting. This PC is a contrast between economy, family and health on one hand and freedom, generosity and trust on the other. This can be interpreted as suggesting that there are two classes of predictors for this data, one I will call goodness of life predictors: economy, family and health, the other goodness of government: freedom, generosity and trust. This indicated that we must be very careful in selecting more than one predictor from each class.

The final PC:
```{r} 
kable(pca$loading[,3], caption= "PCA 3", booktabs=T)
```

is another contrast, this time mixing the two classes that were identified from the second PC. The difference in classification is for generosity.

###Bi-Plot
```{r}
#Do a Biplot to start to look at more relationships between predictors
biplot(pca)
```
To check the conclusions from the principle component analysis(PCA), a bi-plot was constructed. The bi-plot results are very similar to those seen from PCA, the goodness of life predictors are clustered, as are the goodness of government, with generosity further from the other two goodness of government measures.

```{r}
biplot(pca, xlabs=happy[,1])
```
A second bi-plot was constructed to determine which countries could be seen as outliers so that they could be better understood. We immediately see a number of countries which deserve further scrutiny: Myanmar, Rwanda, Somalia, Greece, Lithuania, and Taiwan (hidden by the goodness of life predictors).

####Myanmar
Myanmar is ranked low, 114 out of 155. Its outlier nature is due to its much higher then expected generosity score given its happiness-- it has the highest score. The most likely explanation for this discrepancy is the highly Therevadan Buddhist nature of the majority Bamar ethnic group. 

####Rwanda
Rwanda is also ranked very low, 151 out of 155. It's outlier nature is due to its very high government score given its happiness, the second highest. This is most likely due to the after effects of the Rwandan genocide, and the extreme changes it brought to the Rwandan government.

####Somalia
Somalia is ranked near the middle in terms of happiness, 93. However, Somalia's economy score is second to last, which would usually indicate an extremely low happiness rating. The reason for this difference is most likely the continued factional nature of the Somali state.

####Greece
Greece has a middling happiness level, ranked 87, which is in sharp contrast to its generosity, ranked last, which would usually indicate a much lower happiness. This is most likely a consequence of the continued financial instability of the country.

####Lithuania
Lithuania is ranked in the top third of countries, which is surprising give its low generosity score, only slightly higher than Greece's. This would normally indicate a much less happy nation. Lithuania, like Greece, also recently went through financial instability.

####Taiwan
Taiwan is near the top of the rankings in terms of happiness, ranked 33. However its low freedom score would normally be associated with a much lower happiness. 

###Hierarchical Clustering
```{r}
#hierarchical clustering
happy_h <-hclust(dist(happy[,3:length(happy)]), method="complete")
happy_h$labels <-happy
#Comment out to prevent stray output in pdf
#png(file="Dendrogram_plot.png", width=10, height=24, units="in", res=288)
#par(mar=c(2,2,2,10))
#plot(as.dendrogram(happy_h), horiz=T)
#dev.off()
```
![](dendrogram_plot.png)
\pagebreak

Hierarchical Clustering (HC) was done to better understand the relationship between the different countries, that is, to see if there was some way to classify the countries based on their predictor values. To that end complete clustering was done which produced the dendrogram seen on the previous page.

Interpreting the results, it is the second split that is most meaningful. This splits the countries into three groups: those with very low happiness, the first branch from the bottom, those with high happiness, the second branch, and those with middle happiness, the top branch. 

This 3-way split corresponds, in some way, to the old Cold War, 3 world classification. However, it should be noted that the "3^rd^ world" has greatly shrunk, while the 2^nd^ world previously the domain of communist regimes, has now become the most numerous. The 1st world has also increased in size. 

###K-Means Clustering
```{r}
#k-means clustering
happy_k <- kmeans(happy[,3:length(happy)], 3)
#calculate the first 3 PC
pcc1 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,1]
pcc2 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,2]
pcc3 = as.matrix(happy[,3:length(happy)]) %*% pca$loadings[,3]
#Create a matrix with the PCs and the component membership number
PCs=cbind(pcc1,pcc2,pcc3, happy_k$cluster)

#Plots the first three PCs by cluster with different colors
cluster_1 <- PCs[PCs[,4]==1,]
cluster_2 <- PCs[PCs[,4]==2,]
cluster_3 <- PCs[PCs[,4]==3,]
splt <- scatterplot3d(cluster_1[,1],cluster_1[,2],cluster_1[,3], xlab="PC1", ylab="PC2", zlab="PC3", color = "blue",
                      xlim=c(-2.5,0),ylim=c(-1,1),zlim=c(.2,-.6))
splt$points3d(cluster_2[,1],cluster_2[,2],cluster_2[,3], col="red")
splt$points3d(cluster_3[,1],cluster_3[,2],cluster_3[,3], col="green")

#Put the clusters in as a variable in a new data set to be used in visualizations in tableau
country_clusters <- cbind(happy[,1], happy_k$cluster)
colnames(country_clusters) <- c("Country","Cluster")
write.csv(country_clusters, file="clusters2017.csv")
```
This division of the data into 3 parts is consistent with the results from PCA as well as HC indicates there is some evidence in the data of a tripartite division in the world. For this reason, K-means clustering was done with a cluster number of 3. 

The plot above shows the results of this clustering. Further visualization and interpretation of the cluster will done using Tableau

![](clusters.png)
This plot from Tableau shows a clear indication of the meaning of the 3 clusters identified above. One cluster corresponds to the highest rated countries (green), one the lowest (red), and one to everyone else (yellow).

\pagebreak

##Structured Learning
```{r}
#Since muilticolinearity is an issue, fit 3 models that can help deal with it: linear regression with the LASSO, #random forest and SVM, do LOOCV to determine optimum model compare models based on MSE to select the optimum # model, interpret results in light of how to get the happiest country

#Implement hyperparamter optimization after implementing train/test splitting
#Implemented overfitting check

#Set the number of Train/Test iterations to do
its<-1000
```

###Regression
```{r}
#Use the LASSO to help mitigate multicolinearity
lasso_MSE <- matrix(ncol = 2, nrow=its)
colnames(lasso_MSE) <- c("Train", "Test")

#Use cv.glmnet to find optimum tuning parameter lambda with 5-fold cross validation
lasso_5fold <- cv.glmnet(as.matrix(happy[,3:length(happy)]), happy[,2],nfold=5, family='gaussian')
#plot(lasso_5fold$lambda,lasso_5fold$cvm)
#Record optimum lambda
lambda <- lasso_5fold$lambda.min

for (i in 1:its){
  index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
  train_x <- happy[index,3:length(happy)]
  train_y <- happy[index,2]
  test_x <- happy[-index,3:length(happy)]
  test_y <- happy[-index,2]
  lasso <- glmnet(as.matrix(train_x),train_y, family="gaussian", lambda=lambda)
  
#Testing for overfitting
  preds <- predict(lasso, as.matrix(train_x))
  error <- preds-train_y
  lasso_MSE[i,1] <- mean(error^2)
    
  preds <- predict(lasso, as.matrix(test_x))
  error <- preds-test_y
  lasso_MSE[i,2] <- mean(error^2)
}


```
In order to help overcome issues with the multicolinearity inherent in the data, the LASSO will be used to obtain a linear model of the data. Using 5-fold cross validation to select the optimum $\lambda$ for fitting a LASSO model. Following that a LASSO model was fit to the data. `r its` 70/30 train test splits were performed to estimate the MSE and standard deviation of the MSE for both the training and test data. The results are as follows:
```{r}
kable(apply(lasso_MSE, 2, mean), caption="LASSO MSE Means", booktabs=T)
```
```{r}
kable(apply(lasso_MSE, 2, sd), caption="LASSO MSE Std.", booktabs=T)
```

###Random Forest
```{r}
#Do 1000 70/30 train test splits, average over results to get an idea of ASE
#Create containers for MSE and importance
forest_MSE <- matrix(ncol = 2, nrow=its)
colnames(forest_MSE) <- c("Train", "Test")

forest_importance <- matrix(nrow=its, ncol=length(happy[,3:length(happy)]))
colnames(forest_importance) <- c("Economy", "Family", "Health", "Freedom"
                    , "Generosity", "Trust")
for (i in 1:its){
  index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
  train_x <- happy[index,3:length(happy)]
  train_y <- happy[index,2]
  test_x <- happy[-index,3:length(happy)]
  test_y <- happy[-index,2]
  forest <- randomForest(x=train_x ,y=train_y, xtest=test_x, ytest=test_y,ntree=500, importance=TRUE)
  
  error <- forest$predicted-train_y
  forest_MSE[i,1] <- mean(error^2)
  
  error <- forest$test$predicted-test_y
  forest_MSE[i,2] <- mean(error^2)
  
  forest_importance[i,] <- forest$importance[,1]
}
```
While a random forest is said to be self-validating, given the nature of the algorithm, `r its` 70/30 train/test splits were done to estimate the MSE and $\sigma$ of the estimates. 500 trees were used. Once hyperparameter optimization is coded, this number will be determined by optimization of the fit of the data. The results are as follows:

```{r}
kable(apply(forest_MSE, 2, mean), caption="Random Forest MSE Means", booktabs=T)
kable(apply(forest_MSE, 2, sd), caption="Random Forest MSE Std.", booktabs=T)
```


Furthermore, variable importance was extracted from the random forest model. The results are as follows:
```{r}
kable(apply(forest_importance, 2, mean), caption="Variable Importance Means", booktabs=T)
kable(apply(forest_importance, 2, sd), caption= "Variable Importance Std.", booktabs=T)
```

It is clear from the tables that Health, then Economy, then Family and finally Freedom, in that order play an important role in predicting happiness in a country. Generosity and trust appear to lack much predictive power.
INTERPRET MORE

###Support Vector Machine
```{r}
support_MSE <- matrix(ncol = 2, nrow=its)
colnames(support_MSE) <- c("Train", "Test")

#Try a radial SVM to start, look at others during hyperparameter optimization
for (i in 1:its){
  index <- sample(length(happy[,1]), size=length(happy[,1])*.7)
  train_x <- happy[index,3:length(happy)]
  train_y <- happy[index,2]
  test_x <- happy[-index,3:length(happy)]
  test_y <- happy[-index,2]
  support <- svm(x=train_x ,y=train_y, kernel= "linear")
  
  support_MSE[i,1] <- mean(support$residuals^2)
  
  preds <- predict(support, test_x)
  error <- preds-test_y
  support_MSE[i,2] <- mean(error^2)
}
#Radial kernel overfits, try a different one. Linear fits better and doesn't overfit.
```
Two SVMs were fit to the data. The first was an SVM with radial kernel. This model was found to overfit the data and so was replaced with a model with linear kernel. This model did not overfit the data. `r its` 70/30 train/test splits were done to estimate the MSE and the $\sigma$ of the estimates. No hyperparameter optimization has as of yet been coded.

The following results were found:
```{r}
kable(apply(support_MSE, 2, mean), caption="Support Vector Machine MSE Means", booktabs=T)
kable(apply(support_MSE, 2, sd), caption="Support Vector Machine MSE Std.", booktabs=T)
```


###Optimum Model


\pagebreak

##Conclusions
